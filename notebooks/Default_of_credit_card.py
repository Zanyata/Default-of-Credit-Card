# -*- coding: utf-8 -*-
"""20250113 Kopia notatnika 20250112 Kopia notatnika 20250111 Kopia notatnika Default-of-Credit-Card.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zYnQM8_ag-b4a9FgQOmU7B9mYdy-cV_n

# **Project Name**    - Credit Card Default Prediction

**Project Type**    - Binary Classification

 This script contains a comprehensive analysis for predicting credit card defaults using a binary classification approach. It includes data loading, exploratory data analysis, feature engineering, model training, and evaluation.


 **data**    - https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset

## Features Description

This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. There are 25 features in the dataset:

* **ID**: ID of each client

* **LIMIT_BAL**: Amount of given credit in NT dollars (includes individual and family/supplementary credit)

* **SEX**: Gender (1=male, 2=female)

* **EDUCATION**: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)

* **MARRIAGE**: Marital status (1=married, 2=single, 3=others)

* **AGE**: Age in years

* **PAY_0**: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, 8=payment delay for eight months, 9=payment delay for nine months and above)

* **PAY_2**: Repayment status in August, 2005 (scale same as above)

* **PAY_3**: Repayment status in July, 2005 (scale same as above)

* **PAY_4**: Repayment status in June, 2005 (scale same as above)

* **PAY_5**: Repayment status in May, 2005 (scale same as above)

* **PAY_6**: Repayment status in April, 2005 (scale same as above)

* **BILL_AMT1**: Amount of bill statement in September, 2005 (NT dollar)

* **BILL_AMT2**: Amount of bill statement in August, 2005 (NT dollar)

* **BILL_AMT3**: Amount of bill statement in July, 2005 (NT dollar)

* **BILL_AMT4**: Amount of bill statement in June, 2005 (NT dollar)

* **BILL_AMT5**: Amount of bill statement in May, 2005 (NT dollar)

* **BILL_AMT6**: Amount of bill statement in April, 2005 (NT dollar)

* **PAY_AMT1**: Amount of previous payment in September, 2005 (NT dollar)

* **PAY_AMT2**: Amount of previous payment in August, 2005 (NT dollar)

* **PAY_AMT3**: Amount of previous payment in July, 2005 (NT dollar)

* **PAY_AMT4**: Amount of previous payment in June, 2005 (NT dollar)

* **PAY_AMT5**: Amount of previous payment in May, 2005 (NT dollar)

* **PAY_AMT6**: Amount of previous payment in April, 2005 (NT dollar)

* **default payment next month**: Default payment (1=yes, 0=no)

## ***1. Know Your Data***

### Import Libraries
"""

import pandas as pd
import numpy as np
from yellowbrick.target import ClassBalance
import matplotlib.pyplot as plt
import seaborn as sns
import json
import pickle
from sklearn.model_selection import StratifiedShuffleSplit, KFold, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier
from imblearn.combine import SMOTETomek
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.decomposition import PCA
# pd.reset_option('display.max_rows')

"""### Raw Dataset Loading"""

# Data loading from GitHub repository
def load_data(file_path):
    """Load the dataset from a specified file path."""
    try:
        df = pd.read_csv(file_path, index_col=0)
        return df
    except Exception as e:
        print(f"Error loading data: {e}")
        return None

file_path = 'https://raw.githubusercontent.com/Zanyata/Default-of-Credit-Card/refs/heads/main/data/UCI_Credit_Card.csv'
df = load_data(file_path)

"""### Dataset First View"""

df.head(5)

df.info()

"""No null values in dataset. All numerical"""

df.shape

df.describe()

# Number of duplicates
df.duplicated().sum()

"""There are 35 suplicates which needs to be deleted."""

# Visualizing labels distribution
visualizer = ClassBalance(labels = ["no", "yes"])
visualizer.fit(df["default.payment.next.month"])
visualizer.show()

# Percentage of 1-yes labels
print((((df["default.payment.next.month"]==1).sum()/((df["default.payment.next.month"]==0).sum())))*100,"%")

"""For the dependent variables, there are far more not default payments, which will be additional challenge for ML optimization. This is very unbalanced dataset."""

# Visualizing education and marriage
print("EDUCATION")
visualizer = ClassBalance(labels=["0", "graduate school", "university", "high school", "others", "unknown", "unknown"])
visualizer.fit(df["EDUCATION"])
visualizer.show()

print("MARRIAGE")
visualizer = ClassBalance(labels = ["0", "married", "single", "others"])
visualizer.fit(df["MARRIAGE"])
visualizer.show()

"""There are cases of 0 as education/marriage value, which is incorrect with the features description. There are also two unknown groups and one others in marriage. These categorical features needs correction."""

# Dropping duplicates
df_prep = df.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=True)

# Mapping
# Marriage - changing 0 to others
# Education changing 0, 5=unknown, 6=unknown, 4=others

def mapping(df):
  dict_map_edu = {
      0:4,1:1,2:2,3:3,4:4,5:4,6:4
  }
  dict_map_mar = {
      0:3,1:1,2:2,3:3
  }
  df["EDUCATION"] = df["EDUCATION"].map(dict_map_edu)
  df["MARRIAGE"] = df["MARRIAGE"].map(dict_map_mar)
  return df
df_prep = mapping(df)

"""* Dataset has 30000 samples and 24 features, all numerical
* 35 duplicates
* Columns SEX, MARRIAGE, EDUCATION contains categorical values
* There are no null, but there are some 0 values which aren't provided in data information, they were checked and reassigned
* **Default payment next month** is the dependent variable while the rest is independend variable
* For the dependent variables, there are far more not default payments, which will be additional challenge for ML optimization

## ***2. Data Visualization, Storytelling & Experimenting with charts : Understand the relationships between variables***

###Gender chart
"""

# SEX: Gender (1=male, 2=female)

# Calculating percentage of male and female
classes=df_prep['SEX'].value_counts()
male=classes[1]/df_prep['SEX'].count()*100
female=classes[2]/df_prep['SEX'].count()*100

# Plotting the bar chart
plt.figure(figsize=(8, 6))
plt.bar(['female','male'], classes, color=['lightgreen','yellowgreen'])
plt.title("GENDER Distribution")
plt.annotate("{0:.4}%".format(female),(0.2, 0.5), xycoords='axes fraction')
plt.annotate("{0:.4}%".format(male),(0.7, 0.5), xycoords='axes fraction')
plt.show()

"""Gender isn't balanced, there are more female representatives in dataset.

###Customer default analysis as per Gender
"""

# Simple Customer default analysis as per Gender
pd.crosstab(df_prep["SEX"], df_prep["default.payment.next.month"]).plot(kind="bar",figsize=(8,6),color=['salmon','lightblue'])
plt.title('Payments by Gender')
plt.xlabel(' ')
plt.ylabel('Count of Payments')
plt.legend(['No default','Default'])
plt.xticks(ticks=[0, 1], labels=['Male', 'Female'], rotation=0)
plt.show()

gender_counts = df_prep.groupby('SEX')['default.payment.next.month'].count()  # Total counts by gender
default_counts = df_prep.groupby('SEX')['default.payment.next.month'].sum()  # Default counts by gender
# Calculate percentages
default_percentages = (default_counts / gender_counts) * 100

# Creating the percentage bar chart
plt.figure(figsize=(8, 6))
default_percentages.plot(kind='bar', color=['lightblue'], edgecolor='black')
plt.title('Default Payments as Percentage of Gender')
plt.xlabel('Gender')
plt.ylabel('Default Percentage')
plt.annotate("{0:.4}%".format(default_percentages[1]),(0.2, 0.5), xycoords='axes fraction')
plt.annotate("{0:.4}%".format(default_percentages[2]),(0.7, 0.5), xycoords='axes fraction')
plt.xticks(ticks=[0, 1], labels=['Male', 'Female'], rotation=0)
plt.grid(axis='y', alpha=0.75)
plt.legend(['Default'])
plt.show()

"""Absolute value of female defaulter is more than male defaulter.

Although there is more female defaulters, there are slightly more male defaulters in sex percentage distribution.

###Marital status chart
"""

# Marital status percentage count
classes=df_prep['MARRIAGE'].value_counts()
married=classes[1]/df_prep['MARRIAGE'].count()*100
single=classes[2]/df_prep['MARRIAGE'].count()*100
others=classes[3]/df_prep['MARRIAGE'].count()*100

# Plotting marital status
plt.figure(figsize=(8, 6))
plt.bar(['single', 'married','others'], classes, color='moccasin')
plt.title("MARRIAGE Distribution")
plt.annotate("{0:.4}%".format(single),(0.15, 0.5), xycoords='axes fraction')
plt.annotate("{0:.4}%".format(married),(0.45, 0.5), xycoords='axes fraction')
plt.annotate("{0:.3}%".format(others),(0.8, 0.5), xycoords='axes fraction')
plt.show()

"""###Customer default analysis as per marital status"""

# Simple visualization of marital status and default payment
pd.crosstab(df_prep["MARRIAGE"], df_prep["default.payment.next.month"]).plot(kind="bar",figsize=(8,6),color=['salmon','lightblue'])
plt.title('Payments by Marital Status')
plt.xlabel(' ')
plt.ylabel('Count of Payments')
plt.legend(['No default','Default'])
plt.xticks(ticks=[0, 1, 2], labels=['Married', 'Single', "Others"], rotation=0)
plt.show()

# Marital status vs default percentage count
marital_counts = df_prep.groupby('MARRIAGE')['default.payment.next.month'].count()  # Total counts by gender
default_counts = df_prep.groupby('MARRIAGE')['default.payment.next.month'].sum()  # Default counts by gender
default_percentages = (default_counts / marital_counts) * 100

# Default Payments as Percentage of Marital Status
plt.figure(figsize=(8, 6))
default_percentages.plot(kind='bar', color=['lightblue'], edgecolor='black')
plt.title('Default Payments as Percentage of Marital Status')
plt.xlabel('Marital Status')
plt.ylabel('Default Percentage')
plt.annotate("{0:.4}%".format(default_percentages[1]),(0.15, 0.5), xycoords='axes fraction')
plt.annotate("{0:.4}%".format(default_percentages[2]),(0.45, 0.5), xycoords='axes fraction')
plt.annotate("{0:.4}%".format(default_percentages[3]),(0.8, 0.5), xycoords='axes fraction')
plt.xticks(ticks=[0, 1, 2], labels=['Married', 'Single', "Others"], rotation=0)
plt.grid(axis='y', alpha=0.75)
plt.legend(['Default'])
plt.show()

"""There isn't a significant correlation between default payments and marital status.

###Education
"""

# Simple visualization of education and default payment
pd.crosstab(df_prep["EDUCATION"], df_prep["default.payment.next.month"]).plot(kind="bar",figsize=(8,6),color=['salmon','lightblue'])
plt.title('Payments by Education')
plt.xlabel(' ')
plt.ylabel('Count of Payments')
plt.legend(['No default','Default'])
plt.xticks(ticks=[0, 1, 2, 3], labels=['graduate school', 'university', "high school", "Others"], rotation=0)
plt.show()

# Education vs default percentage count
education_counts = df_prep.groupby('EDUCATION')['default.payment.next.month'].count()  # Total counts by gender
default_counts = df_prep.groupby('EDUCATION')['default.payment.next.month'].sum()  # Default counts by gender
default_percentages = (default_counts / education_counts) * 100

# Default Payments as Percentage of Education
plt.figure(figsize=(8, 6))
default_percentages.plot(kind='bar', color=['lightblue'], edgecolor='black')
plt.title('Default Payments as Percentage of Education')
plt.xlabel('Education')
plt.ylabel('Default Percentage')
plt.annotate("{0:.4}%".format(default_percentages[1]),(0.1, 0.5), xycoords='axes fraction')
plt.annotate("{0:.4}%".format(default_percentages[2]),(0.35, 0.5), xycoords='axes fraction')
plt.annotate("{0:.4}%".format(default_percentages[3]),(0.6, 0.5), xycoords='axes fraction')
plt.annotate("{0:.3}%".format(default_percentages[4]),(0.85, 0.5), xycoords='axes fraction')
plt.xticks(ticks=[0, 1, 2, 3], labels=['graduate school', 'university', "high school", "Others"], rotation=0)
plt.grid(axis='y', alpha=0.75)
plt.legend(['Default'])
plt.show()

"""Absolute numbers of defaulters is maximum among university graduates.

The highest probability of default payment is among high school students.

###Balance Limit correlation
"""

# Histogram to visualize the distribution of 'LIMIT_BAL'

# Plot histogram for all Balance Limit
plt.figure(figsize=(8, 6))
plt.hist(df['LIMIT_BAL'], bins=20, color='orange', edgecolor='black', alpha=0.6, label='Balance limit')

# Plot histogram for defaults only
plt.hist(df[df['default.payment.next.month'] == 1]['LIMIT_BAL'], bins=20, color='deepskyblue', edgecolor='black', alpha=0.6, label='Defaults')

# Add titles and labels
plt.title('Histogram of Balance Limit with Defaults')
plt.xlabel('Balance Limit')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)
plt.legend()  # Add legend
plt.show()

# Boxplot to detect outliers and see spread
plt.figure(figsize=(8, 6))
sns.boxplot(x=df['LIMIT_BAL'], color='orange')
plt.title('Boxplot of LIMIT_BAL')
plt.xlabel('LIMIT_BAL')
plt.show()

# Violin plot for detailed distribution and density
plt.figure(figsize=(8, 6))
sns.violinplot(x=df['LIMIT_BAL'], color='orange')
plt.title('Violin Plot of LIMIT_BAL')
plt.xlabel('LIMIT_BAL')
plt.show()

"""Histogram is right skewed. Visible cluster of more common credit limits on the lower side. From Boxplot we see IQR (50% of all data) ~50-250k. Peaks in the violin plot indicate that values are concentrated ~50k.

Frequency of default payments decreases with lincreasing balance limit.
"""

# Create equal-frequency bins for LIMIT_BAL for ML
df_prep['LIMIT_BAL_bins'] = pd.qcut(df_prep['LIMIT_BAL'], q=5)

# Calculate default rates for each bin
default_rates = df_prep.groupby('LIMIT_BAL_bins')['default.payment.next.month'].mean() * 100  # Convert to percentage

# Count total entries in each bin (optional for understanding bin sizes)
bin_counts = df_prep['LIMIT_BAL_bins'].value_counts()

# Plot default rates by LIMIT_BAL bins
plt.figure(figsize=(10, 6))
default_rates.plot(kind='bar', color='skyblue', edgecolor='black')
plt.title('Default Rates by LIMIT_BAL Bins (Equal-Frequency)')
plt.xlabel('LIMIT_BAL Bins')
plt.ylabel('Default Rate (%)')
plt.grid(axis='y', alpha=0.75)
plt.xticks(rotation=0)
plt.legend(['Default'])
plt.show()

# Print bin counts for review
print("\nNumber of entries in each LIMIT_BAL bin:")
print(bin_counts)

# Converting bins to categorical ((OHE)/SMOTE can work on interval features created after binning, but it requires some preprocessing).
df_prep['LIMIT_BAL_bins'] = df_prep['LIMIT_BAL_bins'].astype('category').cat.codes # If converted to string earlier then category, then int8

"""The default rate is highest in Bin 1 (the lowest credit limit group) and gradually decreases as credit limits increase. This suggests a negative correlation between credit limit (LIMIT_BAL) and default **likelihood**.

###Age
"""

# Histogram to visualize the distribution of 'AGE'
# Plot histogram for all AGE
plt.hist(df['AGE'], bins=10, color='lightgreen', edgecolor='black', alpha=0.6, label='Age')

# Plot histogram for defaults only
plt.hist(df[df['default.payment.next.month'] == 1]['AGE'], bins=20, color='deepskyblue', edgecolor='black', alpha=0.6, label='Defaults')

# Add titles and labels
plt.title('Histogram of Age with Defaults')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)
plt.legend()  # Add legend
plt.show()

# Boxplot to detect outliers and see spread
plt.figure(figsize=(8, 6))
sns.boxplot(x=df['AGE'], color='lightgreen')
plt.title('Boxplot of AGE')
plt.xlabel('AGE')
plt.show()

# Violin plot for detailed distribution and density
plt.figure(figsize=(8, 6))
sns.violinplot(x=df['AGE'], color='lightgreen')
plt.title('Violin Plot of AGE')
plt.xlabel('AGE')
plt.show()

"""The most clients are between 25-30 years. Maximum number of defaulting customers are between 25-30. Visible cluster of more common age  on the lower side. From Boxplot we see IQR (50% of all data) ~27-41 years."""

# Create equal-frequency bins for LIMIT_BAL for ML
df_prep['AGE_bins'] = pd.qcut(df_prep['AGE'], q=5)

# Calculate default rates for each bin
default_rates = df_prep.groupby('AGE_bins')['default.payment.next.month'].mean() * 100  # Convert to percentage

# Count total entries in each bin (optional for understanding bin sizes)
bin_counts = df_prep['AGE_bins'].value_counts()

# Plot default rates by LIMIT_BAL bins
plt.figure(figsize=(10, 6))
default_rates.plot(kind='bar', color='skyblue', edgecolor='black')
plt.title('Default Rates by AGE Bins (Equal-Frequency)')
plt.xlabel('Age Bins')
plt.ylabel('Default Rate (%)')
plt.grid(axis='y', alpha=0.75)
plt.xticks(rotation=90)
plt.legend(['Default'])
plt.show()

# Print bin counts for review
print("\nNumber of entries in each LIMIT_BAL bin:")
print(bin_counts)

# Converting bins to categorical ((OHE)/SMOTE can work on interval features created after binning, but it requires some preprocessing).
df_prep['AGE_bins'] = df_prep['AGE_bins'].astype('category').cat.codes # If converted to string earlier then category, then int8

"""Default payment likelihood is the highest for 21-25. After age 40 likelihood have a constant possitive correlation.

* Columns SEX, MARRIAGE, EDUCATION contains categorical values
* It's unbalnced by genter (more female), education (more university)
* It's rather balanced when it comes to marital status (maried/single)
* LIMIT_BAL: Histogram is right skewed. Visible cluster of more common credit limits on the lower side. From Boxplot we see IQR (50% of all data) ~50-250k. Peaks in the violin plot indicate that values are concentrated ~50k.
* AGE: The most clients are between 25-30 years.Visible cluster of more common age on the lower side. From Boxplot we see IQR (50% of all data) ~27-41 years

###Heatmap
"""

# First Correlation Heatmap visualization code (mapped data & created binns for age and limit_bal)
corrmat = df_prep.corr(numeric_only=True)
plt.figure(figsize=(20,10))
sns.heatmap(df_prep[corrmat.index].corr(),annot=True,linewidths=0.5,fmt='.2f',cmap="RdYlGn")

"""Key Variables Related to Default Payment (default.payment.next.month)


**Top correlated variables:**

PAY_x:
* These represent the repayment status in previous months.
* Positive correlation: Higher delinquency levels in repayment history increase the likelihood of default.
* The highest correlation has the most recent month

LIMIT_BAL (correlation ≈ -0.15):
* Negative correlation suggests customers with higher credit limits are less likely to default, accurate with earlier findings.



**Strong Relationships Among Predictors:**

PAY_x, BILL_ATM_x:

* These represent statuses across months and are strongly correlated since they track similar behaviors over time.
* This means they carry redundant information, leading to multicollinearity.
* Multicollinearity can negatively impact models like logistic regression by inflating coefficients and reducing interpretability.
* Summarization or dimensionality reduction (e.g., PCA) might help.


**Further possibilities**
>Dimensionality reduction helps summarize highly correlated variables (e.g., PAY_x, BILL_AMTx, and potentially PAY_AMTx) into fewer, independent components.

>SMOTE addressing imbalanced datasets by oversampling the minority class.

## ***3. Feature Importances & Dimensionality reduction***

###Feature Importances
"""

# Splitting the dataset with preserving initial data imbalance
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

for train_index, test_index in split.split(df_prep, df_prep["default.payment.next.month"]):
    strat_train_set = df_prep.loc[train_index]
    strat_test_set = df_prep.loc[test_index]

X_train = strat_train_set.drop(columns=["default.payment.next.month"]).values
y_train = strat_train_set["default.payment.next.month"].values

X_test = strat_test_set.drop(columns=["default.payment.next.month"]).values
y_test = strat_test_set["default.payment.next.month"].values

# Simple Random Forest for feature importances
rf = RandomForestClassifier(class_weight='balanced', random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

# Reviewing feture importances for basic dataset
importance = rf.feature_importances_
feature_names = strat_train_set.drop(columns=["default.payment.next.month"]).columns
feature_importance_dict = dict(zip(feature_names, importance))
sorted_importances = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)
print(sorted_importances)

# Visualizing feature importances
plt.figure(figsize=(10, 6))
plt.barh(
    [x[0] for x in sorted_importances],
    [x[1] for x in sorted_importances],
    color='skyblue'
)
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.title("Feature Importances in Random Forest")
plt.gca().invert_yaxis()  # Flip the y-axis to show the most important features at the top
plt.show()

"""We can delete features with lower feature importances than 2%, since we have a lot of features. We see their low importance with random forest analysis and no correlation between those categorical data (SEX, MARRIAGE, EDUCATION) and default payment on heatmap and during EDA with charts. Unfortunately, grouping LIMIT_BAL and AGE wasn't helpful and performed worse than raw features and also will be deleted. PAY_5 and PAY_6 shows the worst correlation thought whole PAY_x features so we can also exclude them without missing important data.

###Dimensionality reduction
"""

columns_to_remove= []
for i in sorted_importances:
  if i[1] < 0.02:
    columns_to_remove.append(i[0])

df_prep = df_prep.drop(columns=columns_to_remove)

"""###Dataset Save"""

# Dataset for further analysis
df_prep.to_csv("df_prep.csv", index=False)

"""##***4. Data load/split***

### Dataset Load
"""

def load_data(file_path):
    """Load the dataset from a specified file path."""
    try:
        df = pd.read_csv(file_path)
        return df
    except Exception as e:
        print(f"Error loading data: {e}")
        return None

"""###Splitting"""

def stratify_splitting(df):
  split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
  try:
    for train_index, test_index in split.split(df, df["default.payment.next.month"]):
      strat_train_set = df.loc[train_index]
      strat_test_set = df.loc[test_index]
    feature_names = strat_train_set.drop(columns=["default.payment.next.month"]).columns
    return strat_train_set, strat_test_set, feature_names
  except Exception as e:
    print(f"Error while splitting: {e}")
    return None

def xy_splitting_scaling(strat_train_set, strat_test_set):
  try:
    X_train = strat_train_set.drop(columns=["default.payment.next.month"])
    y_train = strat_train_set["default.payment.next.month"].values # Convert to NumPy array
    X_test = strat_test_set.drop(columns=["default.payment.next.month"])
    y_test = strat_test_set["default.payment.next.month"].values
    sc = StandardScaler()
    X_train = sc.fit_transform(X_train)
    X_test = sc.transform(X_test)
    return X_train, y_train, X_test, y_test
  except Exception as e:
    print(f"Error while xy_splitting: {e}")

file_path = 'https://raw.githubusercontent.com/Zanyata/Default-of-Credit-Card/refs/heads/main/data/df_prep.csv'
df_prep = load_data(file_path)
strat_train_set, strat_test_set, feature_names = stratify_splitting(df_prep)
X_train, y_train, X_test, y_test = xy_splitting_scaling(strat_train_set, strat_test_set)

"""#***5. AutoML***

AutoML allows testing multiple machine learning models on a dataset, automatically comparing their performance. It selects the best-performing model by evaluating metrics like accuracy, precision, or other criteria specified by the user.
"""

!pip install flaml
from flaml import AutoML
!pip install catboost
from catboost import CatBoostClassifier

"""F1-Score AutoML"""

# Eestimator list
estimator_list = ['xgboost', 'rf', 'lgbm', 'lrl1', 'lrl2', 'catboost', 'svc']

# AutoML settings
automl_settings = {
    "time_budget": 300,  # Run for x sec
    "metric": "f1",  # Optimize for F1-Score
    "task": "classification",  # Classification task
    "estimator_list": estimator_list,  # Custom estimator list
    "log_file_name": "flaml_log.txt",  # Log file for tracking progress
    "n_jobs": -1,  # Use all available CPU cores
    "verbose": 3,  # Detailed logs
    "eval_method": "cv",  # Use cross-validation for robust evaluation
    "log_training_metric": True,  # Log training metrics
}

# Initialize AutoML
automl = AutoML()

# Fit AutoML
automl.fit(X_train=X_train, y_train=y_train, **automl_settings)

# Use the best model for predictions
y_pred = automl.predict(X_test)
y_pred_proba = automl.predict_proba(X_test)[:, 1]  # For ROC-AUC

# Evaluate the results
print("\n=== Model Evaluation ===")
print(f"F1-Score: {f1_score(y_test, y_pred):.4f}")
print(f"Precision: {precision_score(y_test, y_pred):.4f}")
print(f"Recall: {recall_score(y_test, y_pred):.4f}")
print(f"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix for Analysis
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Get the best model
print("\n=== Best Model ===")
print(f"Best Estimator: {automl.best_estimator}")  # Example: 'lgbm' (LightGBM)
print("Best Hyperparameters:")
print(automl.best_config)

# Get the best score
print("\nBest F1-Score (on validation set):")
print(1 - automl.best_loss)

"""ap score AutoML"""

# Estimator list
estimator_list3 = ['xgboost', 'rf', 'lgbm', 'lrl1', 'lrl2', 'catboost', 'svc']

# AutoML settings
automl_settings3 = {
    "time_budget": 300,  # Run for x sec
    "metric": "ap",  # Optimize for Average Precision to minimize False Negatives
    "task": "classification",  # Classification task
    "estimator_list": estimator_list3,  # Custom estimator list
    "log_file_name": "flaml_log3.txt",  # Log file for tracking progress
    "n_jobs": -1,  # Use all available CPU cores
    "verbose": 3,  # Detailed logs
    "eval_method": "cv",  # Use cross-validation for robust evaluation
    "log_training_metric": True,  # Log training metrics
}

# Initialize AutoML
automl3 = AutoML()

# Fit AutoML
automl3.fit(X_train=X_train, y_train=y_train, **automl_settings3)

# Use the best model for predictions
y_pred3 = automl3.predict(X_test)
y_pred_proba3 = automl3.predict_proba(X_test)[:, 1]  # For ROC-AUC

# Evaluate the results
print("\n=== Model Evaluation ===")
print(f"F1-Score: {f1_score(y_test, y_pred3):.4f}")
print(f"Precision: {precision_score(y_test, y_pred3):.4f}")
print(f"Recall (Minimizing False Negatives): {recall_score(y_test, y_pred3):.4f}")
print(f"ROC-AUC: {roc_auc_score(y_test, y_pred_proba3):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred3))

# Confusion Matrix for Analysis
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred3))

# Get the best model
print("\n=== Best Model ===")
print(f"Best Estimator: {automl3.best_estimator}")  # Example: 'lgbm' (LightGBM)
print("Best Hyperparameters:")
print(automl3.best_config)

# Get the best score
print("\nBest Average Precision (on validation set):")
print(1 - automl3.best_loss)

"""AutoML chose LGBM as the best model for evaluating default of credit card dataset, based on F1-Score metric, and Catboost based on ap metric.

#***6. ML***
"""

# Define metrics to evaluate
metrics = [accuracy_score, precision_score, recall_score, f1_score]

# Model training function - for simple model check
def model_training(model, X_train, y_train):
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    preds_proba = model.predict_proba(X_test)[:, 1]  # For ROC-AUC
    return preds, preds_proba

# Evaluation function
def evaluate(preds, preds_proba, metrics, y_test):
    scores = []
    for metric in metrics:
        scores.append(metric(y_test, preds))

    # Compute ROC-AUC score
    roc_auc = roc_auc_score(y_test, preds_proba)
    scores.append(roc_auc)

    # Print classification report
    print("\n=== Classification Report ===")
    print(classification_report(y_test, preds))

    # Confusion Matrix
    conf_matrix = confusion_matrix(y_test, preds)
    return scores, conf_matrix

# Function to plot evaluation results
def plot_evaluation(scores, conf_matrix):
    print("\n=== Evaluation Metrics ===")
    print('Accuracy: '+str(scores[0]))
    print('Precision: '+str(scores[1]))
    print('Recall: '+str(scores[2]))
    print('F1 Score: '+str(scores[3]))
    print('ROC-AUC Score: '+str(scores[4]))

    # Plot confusion matrix with heatmap
    group_names = ['TN', 'FP', 'FN', 'TP']
    group_counts = ["{0:0.0f}".format(value) for value in conf_matrix.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in conf_matrix.flatten() / np.sum(conf_matrix)]
    labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]
    labels = np.asarray(labels).reshape(2, 2)
    plt.figure(figsize=(8, 8))
    sns.heatmap(conf_matrix, annot=labels, fmt='')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

"""### Simple models check"""

# Train and evaluate Logistic Regression
preds, proba = model_training(LogisticRegression(random_state=42, class_weight='balanced'), X_train, y_train)
scores, conf_matrix = evaluate(preds, proba, metrics, y_test)
plot_evaluation(scores, conf_matrix)

# Train and evaluate Random Forest
preds, proba = model_training(RandomForestClassifier(random_state=42, class_weight='balanced'), X_train, y_train)
scores, conf_matrix = evaluate(preds, proba, metrics, y_test)
plot_evaluation(scores, conf_matrix)

# Train and evaluate LightGBM
preds, proba = model_training(LGBMClassifier(random_state=42, class_weight='balanced'), X_train, y_train)
scores, conf_matrix = evaluate(preds, proba, metrics, y_test)
plot_evaluation(scores, conf_matrix)

# Train and evaluate XGBoost
preds, proba = model_training(XGBClassifier(random_state=42, scale_pos_weight=(len(y_train) - sum(y_train)) / sum(y_train)), X_train, y_train)
scores, conf_matrix = evaluate(preds, proba, metrics, y_test)
plot_evaluation(scores, conf_matrix)

# Train and evaluate CatBoost
preds, proba = model_training(CatBoostClassifier(random_state=42, auto_class_weights='Balanced', verbose=0), X_train, y_train)
scores, conf_matrix = evaluate(preds, proba, metrics, y_test)
plot_evaluation(scores, conf_matrix)

"""*Most machine learning models and evaluation metrics in popular libraries like scikit-learn automatically assume that the higher numeric label (e.g., 1 in a binary classification problem with labels 0 and 1) is the positive class by default.*

---

Ranking of Models (Imbalanced Dataset):

**LightGBM**: Best balance of precision, recall, and F1-score; efficient and scalable. Aligns with both F1-score from manual metrics and AutoML’s selection. Best for balanced performance.

**CatBoost**: good F1-score. Handles categorical data natively (not relevant here but advantageous generally). Robust to overfitting.

**Logistic Regression**: Simpler model with reasonable recall (0.63) and interpretability. A good baseline to compare more complex models.

###Fine tuning hyperparameters of chosen models

Based on AutoML Best Hyperparameters let's continue fine tuning to see the best results for LGBM and CatBoost Classifiers.

LGBM:
{'n_estimators': 5, 'num_leaves': 6, 'min_child_samples': 16, 'learning_rate': 1.0, 'log_max_bin': 10, 'colsample_bytree': 0.9364631270378546, 'reg_alpha': 0.039200761358427656, 'reg_lambda': 11.618528403273652}

CatBoost:
{'early_stopping_rounds': 13, 'learning_rate': 0.025858503359419936, 'n_estimators': 8192}

####LGBM
"""

# Model
model = LGBMClassifier(class_weight='balanced')

# Parameter Grid
param_grid = {
    'n_estimators': [20, 30],
    'num_leaves': [6, 8, 10],
    'min_child_samples': [10, 12, 15],
    'learning_rate': [0.2, 0.4, 0.6],
    'max_bin': [8, 10, 12],
    'colsample_bytree': [0.8, 0.936],  # Explore robust feature subsampling,  Increasing to 1 means using all features per tree, which might overfit
    'reg_alpha': [0.02, 0.06, 0.1],  # L1 regularization
    'reg_lambda': [11, 12, 13],  # L2 regularization
    'random_state': [42],  # Keep reproducible
}

# Folds
folds = KFold(n_splits=10, shuffle=True, random_state=42)

# GridSearchCV
GS = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring='f1', #recall roc_auc
    cv=folds,
    return_train_score=True,
    verbose=3,
    n_jobs=-1,  # Use all CPUs
    refit=True
)

# Fit model
GS.fit(X_train, y_train)
preds = GS.predict(X_test)
preds_proba = GS.predict_proba(X_test)[:, 1]

# Best Model and Parameters
best_model = GS.best_estimator_
best_params = GS.best_params_
print(f"Best Parameters: {best_params}")

"""Best Parameters, trained on base dataset: {'colsample_bytree': 0.8, 'learning_rate': 0.4, 'max_bin': 10, 'min_child_samples': 12, 'n_estimators': 30, 'num_leaves': 8, 'random_state': 42, 'reg_alpha': 0.06, 'reg_lambda': 12}"""

# Save model
filename = 'best_LGBM.sav'
pickle.dump(best_model, open(filename, 'wb'))

# Load model & evaluate
filename = 'best_LGBM.sav'
best_LGBM = pickle.load(open(filename, 'rb'))
preds_LGBM = best_LGBM.predict(X_test)
preds_proba_LGBM = best_LGBM.predict_proba(X_test)[:, 1]
scores_LGBM, conf_matrix_LGBM = evaluate(preds_LGBM, preds_proba_LGBM, metrics, y_test)
plot_evaluation(scores_LGBM, conf_matrix_LGBM)

# ROC Curve plot
def plot_roc_curve(y_test, preds_proba):
    fpr, tpr, thresholds = roc_curve(y_test, preds_proba)
    roc_auc = roc_auc_score(y_test, preds_proba)

    plt.figure(figsize=[8, 5])
    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')
    plt.plot([0, 1], [0, 1], ls="--", color='gray')  # Diagonal line
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC-AUC Curve')
    plt.legend(loc='lower right')
    plt.show()

plot_roc_curve(y_test, preds_proba_LGBM)

"""####CatBoost"""

# Model
model = CatBoostClassifier(auto_class_weights='Balanced', early_stopping_rounds=200)

# Parameter Grid
param_grid = {
    'iterations': [300],
    'depth': [5, 7],
    'learning_rate': [0.05, 0.07],
    'l2_leaf_reg': [2, 3, 4],
    'random_strength': [1.5, 2],
    'bagging_temperature': [0.5, 0.7],
    'random_seed': [42],  # Fixed for reproducibility
}


# Folds
folds = KFold(n_splits=5, shuffle=True, random_state=42)

# GridSearchCV
GS = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring='f1',  # Can use recall, roc_auc depending on goal
    cv=folds,
    return_train_score=True,
    verbose=3,
    n_jobs=-1,  # Use all CPUs
    refit=True
)

# Fit model
GS.fit(X_train, y_train)
preds = GS.predict(X_test)
preds_proba = GS.predict_proba(X_test)[:, 1]

# Best Model and Parameters
best_model = GS.best_estimator_
best_params = GS.best_params_
print(f"Best Parameters: {best_params}")

"""Best parameters, trained on base dataset: {'bagging_temperature': 0.5, 'depth': 7, 'iterations': 300, 'l2_leaf_reg': 3, 'learning_rate': 0.05, 'random_seed': 42, 'random_strength': 1.5}

"""

# Save model
filename = 'best_CatBoost.sav'
pickle.dump(best_model, open(filename, 'wb'))

# Load model & evaluate
filename = 'best_CatBoost.sav'
best_CatBoost = pickle.load(open(filename, 'rb'))
preds_CatBoost = best_CatBoost.predict(X_test)
preds_proba_CatBoost = best_CatBoost.predict_proba(X_test)[:, 1]
scores_CatBoost, conf_matrix_CatBoost = evaluate(preds_CatBoost, preds_proba_CatBoost, metrics, y_test)
plot_evaluation(scores_CatBoost, conf_matrix_CatBoost)

# ROC Curve plot
def plot_roc_curve(y_test, preds_proba):
    fpr, tpr, thresholds = roc_curve(y_test, preds_proba)
    roc_auc = roc_auc_score(y_test, preds_proba)

    plt.figure(figsize=[8, 5])
    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')
    plt.plot([0, 1], [0, 1], ls="--", color='gray')  # Diagonal line
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC-AUC Curve')
    plt.legend(loc='lower right')
    plt.show()

plot_roc_curve(y_test, preds_proba_CatBoost)

"""####Logistic Regression"""

# Model
model = LogisticRegression(class_weight = 'balanced', verbose=1)

# Parameter Grid
param_grid = {
    'penalty': ['l2'],
    'solver': ['lbfgs', 'saga'],
    'max_iter': [100, 500, 1000],
    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength, smaller values indicate stronger regularization
    'random_state': [42],  # Reproducibility
}

# Folds
folds = KFold(n_splits=10, shuffle=True, random_state=42)

# GridSearchCV
GS = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring='f1',  # Can use recall, roc_auc depending on goal
    cv=folds,
    return_train_score=True,
    verbose=3,
    n_jobs=-1,  # Use all CPUs
    refit=True
)

# Fit model
GS.fit(X_train, y_train)
preds = GS.predict(X_test)
preds_proba = GS.predict_proba(X_test)[:, 1]

# Best Model and Parameters
best_model = GS.best_estimator_
best_params = GS.best_params_
print(f"Best Parameters: {best_params}")

# Save model
filename = 'best_logr.sav'
pickle.dump(best_model, open(filename, 'wb'))

# Load model & evaluate
filename = 'best_logr.sav'
best_logr = pickle.load(open(filename, 'rb'))
preds_logr = best_logr.predict(X_test)
preds_proba_logr = best_logr.predict_proba(X_test)[:, 1]
scores_logr, conf_matrix_logr = evaluate(preds_logr, preds_proba_logr, metrics, y_test)
plot_evaluation(scores_logr, conf_matrix_logr)

# ROC Curve plot
def plot_roc_curve(y_test, preds_proba):
    fpr, tpr, thresholds = roc_curve(y_test, preds_proba)
    roc_auc = roc_auc_score(y_test, preds_proba)

    plt.figure(figsize=[8, 5])
    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')
    plt.plot([0, 1], [0, 1], ls="--", color='gray')  # Diagonal line
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC-AUC Curve')
    plt.legend(loc='lower right')
    plt.show()

plot_roc_curve(y_test, preds_proba_logr)

"""####Summary

In this project 3 models were chosen, based on simple models performance and AutoML findings, both approaches gave similar results. Chosen models: LGBM, CatBoost and Logistic regression for comparison of models performance.

**Logistic Regression**

Lowest precission, which shows high rate of false possitive results. Simple classification model struggles to differenciate default and nondefault  cases. Recall almost as good as for the best model and low F1 score.

**CatBoost**

The best precision. The worst recall. We see the tradeoff between precison and recall. We see also the best F1 score obtained for this dataset.

**LGBM**

Precision second-best, lower than CatBoosts's for 2.2%. The best recall. This model shows the best capability of detecting minority class. F1 scores almost identical to Catboost, which shows the best compromise between precision and recall for this model.

---
 Hyperparameter tuning was explored for optimization but did not yield substantial improvements. Default settings of LGBM and CatBoost are optimized for general use cases and often provide good performance without extensive tuning.

#***7. SMOTE***

Avoid Data Leakage:

*Fitting scalers, PCA, or SMOTE on the entire dataset would leak information from the test set into the training process, compromising evaluation.*


---
"""

# Data Load
file_path = 'https://raw.githubusercontent.com/Zanyata/Default-of-Credit-Card/refs/heads/main/data/df_prep.csv'
df_prep = load_data(file_path)
strat_train_set, strat_test_set, feature_names = stratify_splitting(df_prep)
X_train_smote, y_train_smote, X_test_nsmote, y_test_nsmote = xy_splitting_scaling(strat_train_set, strat_test_set)

# Print class distributions
print("Before SMOTE + Tomek Links (Training):")
print(np.asarray(np.unique(y_train_smote, return_counts=True)).T)

# Apply SMOTETomek
smote_tomek = SMOTETomek(random_state=42)
X_train_smote, y_train_smote = smote_tomek.fit_resample(X_train_smote, y_train_smote)

# Print class distributions after SMOTE
print("\nAfter SMOTE + Tomek Links (Training):")
print(np.asarray(np.unique(y_train_smote, return_counts=True)).T)
# DO NOT apply SMOTETomek to the test set
print("\nTest set class distribution (untouched):")
print(np.asarray(np.unique(y_test_nsmote, return_counts=True)).T)

# Fixed Hyperparameters from GridSearch best hyperparameters
model = LGBMClassifier(
    colsample_bytree=0.8,
    learning_rate=0.4,
    max_bin=10,
    min_child_samples=12,
    n_estimators=30,
    num_leaves=8,
    random_state=42,
    reg_alpha=0.06,
    reg_lambda=12,
    class_weight='balanced'
)

# Fit the model on SMOTE-balanced training data
model.fit(X_train_smote, y_train_smote)

# Predict outcomes and probabilities on the non-SMOTE test set
preds_LGBM = model.predict(X_test_nsmote)
preds_proba_LGBM = model.predict_proba(X_test_nsmote)[:, 1]

# Evaluate and plot results
scores_LGBM, conf_matrix_LGBM = evaluate(preds_LGBM, preds_proba_LGBM, metrics, y_test_nsmote)
plot_evaluation(scores_LGBM, conf_matrix_LGBM)

"""Insights:
SMOTE Performance:

SMOTE often helps in improving recall for imbalanced datasets but at the cost of precision. However, in this case, recall decreased, which suggests that SMOTE might not have added significant value or may have caused overfitting to the synthetic samples.
The decline in ROC-AUC and F1 further supports the idea that SMOTE wasn’t particularly helpful.

#***8. Additional Data Engineering & PCA***
"""

# Data Load
file_path = 'https://raw.githubusercontent.com/Zanyata/Default-of-Credit-Card/refs/heads/main/data/df_prep.csv'
df_prep = load_data(file_path)
strat_train_set, strat_test_set, feature_names = stratify_splitting(df_prep)

# Correlation Heatmap
corrmat = strat_train_set.corr(numeric_only=True)
plt.figure(figsize=(20,10))
sns.heatmap(strat_train_set[corrmat.index].corr(),annot=True,linewidths=0.5,fmt='.2f',cmap="RdYlGn")

"""###Adding interaction LIMIT_BAL_PAY_0"""

# Investigating Combinations for Stronger Predictive Power
# Interaction Term
strat_train_set['LIMIT_BAL_PAY_0'] = strat_train_set['LIMIT_BAL'] * strat_train_set['PAY_0']
strat_test_set['LIMIT_BAL_PAY_0'] = strat_test_set['LIMIT_BAL'] * strat_test_set['PAY_0']

# Test the correlation of the interaction term with the target variable
interaction_corr = strat_train_set[['LIMIT_BAL_PAY_0', 'default.payment.next.month']].corr()
print("Correlation of Interaction Term with Default Payment:")
print(interaction_corr)

"""This interaction term can be included in the predictive model to enhance performance.

###Logarithic Transformation for skewed data
"""

strat_train_set['log_LIMIT_BAL'] = np.log1p(strat_train_set['LIMIT_BAL'])
strat_test_set['log_LIMIT_BAL'] = np.log1p(strat_test_set['LIMIT_BAL'])

"""###PCA"""

# Addressing Multicollinearity in PAY_x, PAY_AMTx, and BILL_AMTx
# Function to calculate VIF for a DataFrame
def calculate_vif(df):
    vif_data = pd.DataFrame()
    vif_data["feature"] = df.columns
    vif_data["VIF"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]
    return vif_data

# Select columns to test for multicollinearity
features_to_test = df_prep[['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4',
                            'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',
                            'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']]

# Calculate VIF
vif_result = calculate_vif(features_to_test)
print("Variance Inflation Factors:")
print(vif_result)

"""Analysis of Variance Inflation Factors (VIF)
VIF Insights:

BILL_AMTx variables show very high VIF values (above 10), indicating severe multicollinearity.

PAY_x, PAY_AMT_x variables have moderate VIF values, suggesting some multicollinearity but not as severe as BILL_AMTx.
"""

# Define feature groups
PAY_x_BILL_x_PAY_AMTx = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4',
                         'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',
                         'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']

# Standardize features in the training set
scaler = StandardScaler()
train_scaled = scaler.fit_transform(strat_train_set[PAY_x_BILL_x_PAY_AMTx])  # Fit and transform training set
test_scaled = scaler.transform(strat_test_set[PAY_x_BILL_x_PAY_AMTx])        # Transform test set using the same scaler

# Perform PCA on the entire feature set
pca = PCA(n_components=5, random_state=42)  # Adjust number of components as needed
train_pca = pca.fit_transform(train_scaled)  # Fit and transform training set
test_pca = pca.transform(test_scaled)        # Transform test set

# Add PCA components to the datasets
for i in range(train_pca.shape[1]):
    strat_train_set[f'PCA_PAY_x_BILL_x_PAY_AMTx{i+1}'] = train_pca[:, i]
    strat_test_set[f'PCA_PAY_x_BILL_x_PAY_AMTx{i+1}'] = test_pca[:, i]

# Print explained variance ratios
print(f"Explained Variance Ratios for Combined PCA: {pca.explained_variance_ratio_}")

#Replace Original Variables:
# Drop original PAY_x and BILL_AMTx columns with omitting PAY_0 since it showed highest correlation among theirs groups in feature importances
columns_for_removal = ['PAY_2', 'PAY_3', 'PAY_4',
                       'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',
                       'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',
                       'LIMIT_BAL']
strat_train_set = strat_train_set.drop(columns=columns_for_removal)
strat_test_set = strat_test_set.drop(columns=columns_for_removal)

feature_names_pca = strat_train_set.drop(columns=["default.payment.next.month"]).columns
X_train_pca, y_train_pca, X_test_pca, y_test_pca = xy_splitting_scaling(strat_train_set, strat_test_set)

# Correlation Heatmap after PCA
corrmat = strat_train_set.corr(numeric_only=True)
plt.figure(figsize=(10,5))
sns.heatmap(strat_train_set[corrmat.index].corr(),annot=True,linewidths=0.5,fmt='.2f',cmap="RdYlGn")

"""The correlation between log_LIMIT_BAL and the target (default.payment.next.month) is slightly lower than for LIMIT_BAL (-0.18 vs -0.16). This is not necessarily bad if the transformation helps improve model performance by handling outliers."""

# Fixed Hyperparameters from GridSearch best hyperparameters
model = LGBMClassifier(
    colsample_bytree=0.8,
    learning_rate=0.4,
    max_bin=10,
    min_child_samples=12,
    n_estimators=30,
    num_leaves=8,
    random_state=42,
    reg_alpha=0.06,
    reg_lambda=12,
    class_weight='balanced'
)

# Fit the model on PCA training data
model.fit(X_train_pca, y_train_pca)

# Predict outcomes and probabilities on the PCA test set
preds_LGBM = model.predict(X_test_pca)
preds_proba_LGBM = model.predict_proba(X_test_pca)[:, 1]

# Evaluate and plot results
scores_LGBM, conf_matrix_LGBM = evaluate(preds_LGBM, preds_proba_LGBM, metrics, y_test_pca)
plot_evaluation(scores_LGBM, conf_matrix_LGBM)

"""Given the small differences in metrics and the slight decrease in recall (important for detecting minority classes), focusing on the non-PCA approach without extensive data engineering is likely more effective. This approach preserves the interpretability of features and engage lower resources.

#***9. Conclusion***

---
* During this project there was performed Exploratory Data Analysis,  feature
importances, and dimensionality reduction. Some alternative routs of data alteration was also pursued, like SMOTE and PCA.

* The design of this project was to choose the best model which had the capacity of recognition such imbalanced  dataset. As a scoring method was used F1 score, which gives good compromise between precision and recall, which will minimize risk of overfitting. Precision, recall and F1 score were the most important metrics indicating model's overall performance. LGBM was chosen.

* Afterwards such model was tested, whether SMOTE, PCA or additional data engineering had any possitive impact on the evaluation metrics. In such case the chosen model would be trained again on altered data and such model would be used for predictions.

* Supprisingly SMOTE haven't helped, and in fact recall decreased. Also data engineering and PCA wasn't that helpful since model produced only slightly different outcome than base data, which shows that multicollinearity in given data wasn't that prevalent. Using PCA might be taken into consideration in large models/dataset cases, where such obtained dimensionality reduction would save a lot of computational power and time.

* Given all tests and taking into consideration lower computational demand, LGBM stays the best possible model for such dataset, it's efficient and scalable. Aligns with both F1-score from manual metrics and AutoML’s selection. Best for balanced performance. Possible further roads of improvement might contain: pocessing additional features of new samples, threshold adjustment (depending
on business case), models ensembling.

---
"""